---
layout: post
title:  深度学习花书笔记（三）
date:   2019-08-11 
subtitle:   深度前馈网络
author:     CYQ
header-img: img/deep-learning-medicine.jpg
catalog: true
mathjax: true
tags:
  - 深度学习
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

* TOC
{:toc}
------

# 输出单元

## 线性单元

给定特征$\bm{h}$，**线性**输出单元产生一个向量$\hat{\bm{y}}=\bm{W}^\top\bm{h}+\bm{b}$。

该单元往往用于输出一个高斯分布（条件概率）的**均值**$\hat{\bm{y}}$，高斯分布如下：

$$
p(\bm{y}\mid\bm{x})=\mathcal{N}(\bm{y};\hat{\bm{y}},\bm{I})
$$

最大化其对数似然相当于最小化均方误差。

## Sigmoid

给定特征$\bm{h}$，**sigmoid**输出单元产生一个向量$\hat{y}=\sigma\left(\bm{w}^\top\bm{h}+b\right)$。其中$\sigma(x)=\frac{1}{1+\exp(-x)}$。

该单元往往用于输出一个Bernoulli分布，常常是二分类结果的**概率**。假设可能的$y$为0或1，那么将得到$y$的概率分布

$$
P(y)=\sigma\left((2y-1)z\right),z=\bm{w}^\top\bm{h}+b
$$

对于二分类，最终的损失函数一般使用交叉熵，设$y$为正确的标签，即

$$
\begin{split}
J(\bm{\theta})&=-\log P(y\mid\bm{x})\\
&=\zeta((1-2y)z)
\end{split}
$$

其中$\zeta(x)=\log(1+\exp(x))$。

如果使用其他不带对数的损失函数，由于sigmoid在两侧梯度饱和严重，将难以进行学习。

## Softmax

**softmax**是sigmoid的多类别版本。为了表示每个类的概率，输出应当是一个向量$\hat{\bm{y}}$，其每个元素$\hat{y}_i$都在0到1之间，并且各元素和为1

假设前面的线性层输出了未归一化的对数概率

$$
\bm{z}=\bm{W}^\top h+\bm{b}
$$

这里对数概指的是$z_i=\log \hat{P}(y=i\mid\bm{x})$

然后softmax可以对其归一化：

$$
\text{softmax}(\bm{z})_i=\frac{\exp(z_i)}{\sum_j\exp(z_j)}
$$

跟sigmoid一样，使用带对数的损失函数，如交叉熵可以获得很好的效果。这里最小化交叉熵也相当于最大化对数似然：

$$
\log\text{softmax}(\bm{z})_i=z_i-\log\sum_j\exp(z_j)
$$

在训练过程中，最大化似然会导致正确的标签对应的前一项$z_i$被提升，而后一项被缩减。

由于指数爆炸的特性，我们可以把后一项近似看成$\max_jz_j$。如果预测的最大概率对应的类错误，那么梯度下降时会对这个错误预测进行惩罚。

### 关于soft

softmax顾名思义，似乎是$\max$函数的soft版本，然而这是**错误**的。softmax应当是$\arg\max$的soft版本。

一般我们说一个函数的soft版本，指的是做出少许改动之后连续和可微的版本。例如softplus$\zeta(x)=\log(1+\exp(x))$就是**线性整流单元rectified linear unit** (**ReLU**)的软化版本，因此也有别称**SmoothReLU**。

![](/img/Rectifier_and_softplus_functions.svg)

# 隐单元

## 整流线性单元（ReLU）

**整流线性单元**指的是函数

$$
g(z)=\max\{0,z\}
$$

一个明显的优点是：训练方便。因为导数总是常数，并且只要处于激活状态（非负），总有稳定的梯度。然而处于负值时，梯度始终为0.这使得反向传播的梯度无法回传给ReLU之前的单元。

基于此，人们提出了一些变种如Leaky ReLU, PReLU，或者使用软化版本softplus（~~反直觉的是，softplus实际表现得不如ReLU，因此现在大家基本还是用ReLU~~）

## Sigmoid & tanh

这两个函数是类似的，因为两者是线性关系：

$$
\tanh(z)=2\sigma(2z)-1
$$

两者都有梯度饱和的问题，需要时会更多使用$\tanh$，这是因为其在0处接近线性函数。只要激活足够小就很容易训练。

