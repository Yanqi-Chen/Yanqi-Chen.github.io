---
layout: post
title:  深度学习花书笔记（四）
date:   2019-08-12 
subtitle:   正则化
author:     CYQ
header-img: img/Regularization.svg
catalog: true
mathjax: true
header-img-credit: Wikipedia user Nicoguaro
header-img-credit-href: 'https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Regularization.svg'
tags:
  - 深度学习
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

* TOC
{:toc}
------

**正则化**是指修改学习算法，使其降低泛化误差而非训练误差。一个有效的正则化能够在不大幅增加偏差的同时显著减少方差

# 范数惩罚

对于目标函数$J$，常常通过增加范数惩罚$\Omega(\bm{\theta})$，限制其参数的某些规模。将正则化后的目标记为$\hat{J}$

$$
\tilde{J}(\bm{\theta};\bm{X},\bm{y})=J(\bm{\theta};\bm{X},\bm{y})+\alpha\Omega(\bm{\theta})
$$

其中$\alpha$用于权衡范数惩罚项$\Omega(\bm{\theta})$与原始目标函数$J(\bm{X};\bm{\theta})$

需要注意的是，对于权重(weight)与偏置(offset)，一般**不对偏置正则化**，这是因为偏置仅控制一个变量的输出，不对其正则化也不会导致太大的方差。另外，对偏置正则化会导致明显的欠拟合。

基于此，以下区分两个标记：$\bm{w}$表示所有应受范数惩罚的参数的权重，$\bm{\theta}$表示所有参数（含不需正则化的参数）的权重。

## $L^2$正则化

常见的正则化方法，又被称之为**权重衰减、岭回归、Tikhonov正则**。

我们分析一下这个正则化的表现，为了简化，我们假定$\bm{\theta}$就是$\bm{w}$，添加了正则化项后的总目标函数为：

$$
\tilde{J}(\bm{w};\bm{X},\bm{y})=J(\bm{w};\bm{X},\bm{y})+\frac\alpha2\bm{w}^\top \bm{w}
$$

做梯度下降时，对应的梯度为

$$
\nabla_\bm{w}\tilde{J}(\bm{w};\bm{X},\bm{y})=\alpha \bm{w}+\nabla_\bm{w}J(\bm{w};\bm{X},\bm{y})
$$

单步更新为

$$
\begin{split}
&\bm{w}\leftarrow \bm{w}-\epsilon(\alpha\bm{w}+\nabla_\bm{w}J(\bm{w};\bm{X},\bm{y}))\\
\Leftrightarrow &\bm{w}\leftarrow(1-\epsilon\alpha)\bm{w}-\epsilon\nabla_\bm{w}J(\bm{w};\bm{X},\bm{y}))
\end{split}
$$

区别在于更新前会先对$\bm{w}$收缩。这是单步更新的区别，接下来看整体的结果。

设$\bm{w}^\ast$是未正则化的目标函数取最小训练误差时的权重，即$\bm{w}^\ast=\arg\min_\bm{w}J(\bm{w})$，在$\bm{w}^\ast$的邻域对目标函数作二次近似结果如下：

$$
\hat{J}(\bm{\theta})=J(\bm{w}^\ast)+\frac12(\bm{w}-\bm{w^\ast})^\top\bm{H}(\bm{w}-\bm{w}^\ast)\label{approx}\tag{1}
$$

**注意：$\hat{J},J,\tilde{J}$的区分。**

这里说明一下，由于$\bm{w}^\ast$是最优解，这里一阶项（梯度）为0，Hessian矩阵$\bm{H}$半正定。

假定正则化之后最优点为$\tilde{\bm{w}}$，为了避免$\tilde{\bm{w}}$离$\bm{w}$太远导致的近似高阶误差问题，简化假设，**设$J$就是二次的**。那么$\eqref{approx}$的近似就是精确的，此时$\nabla_\bm{w}\tilde{J}(\tilde{\bm{w}})=0$能推出：

$$
\begin{split}
&\alpha\tilde{\bm{w}}+\bm{H}(\tilde{\bm{w}}-\bm{w}^\ast)=0\\
\Leftrightarrow&\tilde{\bm{w}}=(\bm{H}+\alpha\bm{I})^{-1}\bm{Hw}^\ast
\end{split}\label{sol}\tag{2}
$$

当$\alpha\to0$时，$\tilde{\bm{w}}\to\bm{w}^\ast$。而当$\alpha$增大时，由于$\bm{H}$是实对称方阵，对其作正交分解$\bm{H}=\bm{Q\Lambda Q}^\top$,$\eqref{sol}$化为

$$
\begin{split}
\tilde{\bm{w}}&=(\bm{Q\Lambda Q}^\top+\alpha\bm{I})^{-1}\bm{Q\Lambda Q}^\top\bm{w}^\ast\\
&\mathtip{=}{\bm{QQ}^\top=\bm{I}}[\bm{Q}(\bm{\Lambda}+\alpha\bm{I})\bm{Q}^\top]^{-1}\bm{Q\Lambda Q}^\top\bm{w}^\ast\\
&=\bm{Q}(\bm{\Lambda}+\alpha\bm{I})^{-1}\bm{\Lambda}\bm{Q}^\top\bm{w}^\ast
\end{split}
$$

注意到

$$
(\bm{\Lambda}+\alpha\bm{I})^{-1}\bm{\Lambda}=\text{diag}([\frac{\lambda_1}{\lambda_1+\alpha},\cdots,\frac{\lambda_n}{\lambda_n+\alpha}])
$$

其中$\lambda_i$是$\bm{H}$的第$i$个特征值。因此变换的效果是以$\frac{\lambda_i}{\lambda_i+\alpha}$为比例系数缩放$\bm{w}$在$\bm{H}$的第$i$个特征向量上的分量。

从式子上一个直观的感受是$\lambda_i$越大，这个方向上的分量受影响越小。用一幅图可以展示二维情况下的效果：

![l2-ball](/img/l2-ball.svg)

原始目标函数的最优解在椭圆中心，偏水平方向的特征向量较小（等值线较稀疏），因此正则化后偏移得比较远。

## $L^1$正则化

同上，添加了正则化项后的总目标函数为：

$$
\tilde{J}(\bm{w};\bm{X},\bm{y})=J(\bm{w};\bm{X},\bm{y})+\alpha\Vert\bm{w}\Vert_1
$$

梯度下降将得到

$$
\nabla_\bm{w}\tilde{J}(\bm{w};\bm{X},\bm{y})=\alpha\text{sign}( \bm{w})+\nabla_\bm{w}J(\bm{w};\bm{X},\bm{y})
$$

其中$\text{sign}(\cdot)$为符号函数，这个形式似乎很难得到算术解。为了简化，我们仍然采用前的二次近似，并假设$J$就是二次的，使得近似精确，根据$\eqref{approx}$，我们还是可以得到

$$
\nabla_\bm{w}\tilde{J}(\bm{w})=\bm{H}(\tilde{\bm{w}}-\bm{w}^\ast)
$$

然而由于符号函数的出现，我们对接下来形如$\eqref{sol}$的式子束手无策，因此我们进一步**加强假设**，认为Hessian矩阵是对角阵，即$\bm{H}=\text{diag}([H_{1,1},\cdots,H_{n,n}])$，由正定性$H_{i,i}>0$。这个假设可以看作进行过PCA之后的数据，特征已经没有相关性。那么有：

$$
\begin{split}
\tilde{J}(\bm{w};\bm{X},\bm{y})&=J(\bm{w}^\ast;\bm{X},\bm{y})+\frac12(\bm{w}-\bm{w^\ast})^\top\bm{H}(\bm{w}-\bm{w}^\ast)+\alpha\Vert\bm{w}\Vert_1\\
&=J(\bm{w}^\ast;\bm{X},\bm{y})+\sum_i\left[\frac12H_{i,i}(w_i-w_i^\ast)^2+\alpha\vert w_i\vert\right]
\end{split}
$$

这样一来，对各$w_i$求导得到

$$
w_i=\text{sign}(w_i^\ast)\max\left\{|w_i^\ast|-\frac{\alpha}{H_{i,i}},0\right\}
$$

不妨$w_i^\ast>0$（负数类似规律），因此可以分两种情况

1. $w_i^\ast\leq\frac{\alpha}{H_{i,i}}$，那么跟$L^2$正则有点像的是，如果特征值$H_{i,i}$比较小，会导致该方向导致被推离较远。所不同的是，**这里会直接推到0。**
2. $w_i^\ast>\frac{\alpha}{H_{i,i}}$，那么该方向只会移动$\frac{\alpha}{H_{i,i}}$

因此我们说$L^1$**正则可以带来稀疏性**，这种差异是$L^1,L^2$范数球形状的差异导致的。类似$L^2$的一个直观认识如下图：

![l1-ball](/img/l1-ball.svg)

当然，如果Hessian矩阵不是对角阵，那么椭圆的轴方向未必与轴平行，但仍然容易与$L^1$范数等值线的角相交造成稀疏性。

我们前面的[文章]({{ site.baseurl }}{% post_url 2019-08-10-dlbook-1 %})提到了，最大后验估计的视角下，正则化项可以视作某个先验概率密度的对数。这里也是如此，$L^2$正则化可以视为先验**高斯分布**的对数，而$L^1$正则化可视为先验**各向同性拉普拉斯分布**的对数。

$$
\log p(\bm{w})=\sum_i\log\text{Laplace}(w_i;0,\frac{1}{\alpha})=-\alpha\Vert \bm{w}\Vert_1+n\log \alpha-n\log 2
$$

最大化后验概率时可以忽略后面与$\bm{w}$无关的项，相当于添加了$L^1$范数惩罚。

