---
layout: post
title:  神经图灵机
date:   2019-08-26 
subtitle:   Neural Turing Machines
author:     CYQ
header-img: img/Christopher-and-Turing.jpg
catalog: true
mathjax: true
tags:
  - 深度学习
  - 神经图灵机
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

* TOC
{:toc}
------

神经图灵机(NTM)是Google DeepMind 的工作[^graves2014neural]，主要思想是将图灵机中存在的一些离散过程转化为可微分的连续过程，通过神经网络模拟图灵机的工作过程。本质上仍然是RNN的变体。

# 结构

![ntm_arch](/img/ntm_arch.png)

由于模仿的是图灵机，因此NTM有着与图灵机基本类似的架构，主要部件有：一个由神经网络模拟的控制器，一个存储。

控制器的行为同时取决于输入以及存储中读取的信息，通过输出与外界交互的同时也向存储中写入数据。一个图灵机的功能基本上靠的是对控制器设计一个合适的有限状态机(FSM)实现的，这也是NTM中需要采用神经网络模拟的原因之一。

## 工作原理

由于有读写两个不同的过程需要控制，因此需要分别设计。

### 读

记$$t$$时刻的*存储*为$$\bm{M}_t$$，该存储是一个$$N\times M$$的矩阵。其中$$N$$是矩阵中条目的数量，每个条目是一个长为$$M$$的向量。

**读磁头**对应的网络应当输出一个长为$$N$$的权重向量$$\bm{w}_t$$，该向量是归一化的。
$$
\sum_i w_t(i)=1,\qquad 0\leq w_t(i)\leq 1,\forall i.
$$

读磁头读取到的内容是一个长为$M$的向量$$\bm{r}_t$$，跟存储中一个条目的大小相同。我们知道，图灵机往往只离散的读取某一个位置的条目，而为了使得这一过程可微分，NTM读取到的信息设计为所有$N$个位置条目的加权和，即：

$$
\bm{r}_t\longleftarrow\sum_i w_t(i)\bm{M}_t(i)
$$

换言之，每次读磁头的读取都利用了整个内存的信息。权重只是使得读取的选择性变得连续。

### 写

作者此处受LSTM中输入门与遗忘门的启发，将写过程分为两个阶段：*擦除(erase)*和*加(add)*。

#### 擦除

擦除过程指的是将存储中的每个条目按某个系数缩减，不同位置的条目缩减的系数不同，这个系数由**写磁头**得到的一个权重向量$$\bm{w}_t$$，和**擦除向量**$$\bm{e}_t$$共同决定。

$$
\tilde{\bm{M}}_t(i)\longleftarrow \bm{M}_{t-1}(i)\otimes[\bm{1}-w_t(i)\bm{e}_t]
$$

$$\bm{1}$$是一个全1的行向量，$$\otimes$$指的是逐元素相乘。擦除向量长为$$M$$，其元素均位于$$[0,1]$$。

注意到只有当擦除向量为全1，并且权重向量对应位也为1时，这个向量才会被真正擦除为0。另外，这两者只要有一个为0，那么就不会发生擦除。

由于按元素乘运算$$\otimes$$满足*交换律*，因此多个并行擦除操作的*顺序对最终结果没有影响*。

#### 加

如果说前面的擦除只是类似LSTM的遗忘门，那么这里才是真正的写入新数据。

写磁头会生成一个**加数向量**$$\bm{a}_t$$，按权重$$w_t$$加在对应的结果上:

$$
\bm{M}_t(i)\longleftarrow \tilde{\bm{M}}_t(i)+w_t(i)\bm{a}_t
$$

跟按元素乘法一样，这里的加法也满足交换律，因而加的顺序不会造成影响。并且这两个操作都是可微的。

这里的一乘一加两个操作共用了权重$$w_t$$，我简单地将其分量理解为：有多大的意愿修改存储中的这个条目。当其权重的在某个存储位置的分量较大时，将会尽可能用新的内容*取代*该位置上旧的存储内容。

### 寻址机制



完整的寻址流程如下图所示：

![ntm_write](/img/ntm_write.png)



# 参考文献

[^graves2014neural]: Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. *arXiv preprint arXiv:1410.5401*.