---
layout: post
title:  神经图灵机
date:   2019-08-26 
subtitle:   Neural Turing Machines
author:     CYQ
header-img: img/Christopher-and-Turing.jpg
catalog: true
mathjax: true
tags:
  - 深度学习
  - 神经图灵机
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

* TOC
{:toc}
------

神经图灵机(NTM)是Google DeepMind 的工作[^graves2014neural]，主要思想是将图灵机中存在的一些离散过程转化为可微分的连续过程，通过神经网络模拟图灵机的工作过程。本质上仍然是RNN的变体。

# 结构

![ntm_arch](/img/ntm_arch.png)

由于模仿的是图灵机，因此NTM有着与图灵机基本类似的架构，主要部件有：一个由神经网络模拟的控制器，一个存储。

控制器的行为同时取决于输入以及存储中读取的信息，通过输出与外界交互的同时也向存储中写入数据。一个图灵机的功能基本上靠的是对控制器设计一个合适的有限状态机(FSM)实现的，这也是NTM中需要采用神经网络模拟的原因之一。

## 工作原理

由于有读写两个不同的过程需要控制，因此需要分别设计。

<center>
    <img src="/img/rnn_memory.svg">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    font-size: 14px;
    padding: 2px;">图片来自Olah, C., & Carter, S. (2016). Attention and augmented recurrent 	neural networks. Distill, 1(9), e1.</div>
</center>

### 读

记$$t$$时刻的*存储*为$$\bm{M}_t$$，该存储是一个$$N\times M$$的矩阵。其中$$N$$是矩阵中条目的数量，每个条目是一个长为$$M$$的向量。

**读磁头**对应的网络应当输出一个长为$$N$$的权重向量$$\bm{w}_t$$，该向量是归一化的。

$$
\sum_i w_t(i)=1,\qquad 0\leq w_t(i)\leq 1,\forall i.
$$

读磁头读取到的内容是一个长为$M$的向量$$\bm{r}_t$$，跟存储中一个条目的大小相同。我们知道，图灵机往往只离散的读取某一个位置的条目，而为了使得这一过程可微分，NTM读取到的信息设计为所有$N$个位置条目的加权和，即：

$$
\bm{r}_t\longleftarrow\sum_i w_t(i)\bm{M}_t(i)
$$

换言之，每次读磁头的读取都利用了整个内存的信息。权重只是使得读取的选择性变得连续。

### 写

作者此处受LSTM中输入门与遗忘门的启发，将写过程分为两个阶段：*擦除(erase)*和*加(add)*。

#### 擦除

擦除过程指的是将存储中的每个条目按某个系数缩减，不同位置的条目缩减的系数不同，这个系数由**写磁头**得到的一个权重向量$$\bm{w}_t$$，和**擦除向量**$$\bm{e}_t$$共同决定。

$$
\tilde{\bm{M}}_t(i)\longleftarrow \bm{M}_{t-1}(i)\otimes[\bm{1}-w_t(i)\bm{e}_t]
$$

$$\bm{1}$$是一个全1的行向量，$$\otimes$$指的是逐元素相乘。擦除向量长为$$M$$，其元素均位于$$[0,1]$$。

注意到只有当擦除向量为全1，并且权重向量对应位也为1时，这个向量才会被真正擦除为0。另外，这两者只要有一个为0，那么就不会发生擦除。

由于按元素乘运算$$\otimes$$满足*交换律*，因此多个并行擦除操作的*顺序对最终结果没有影响*。

#### 加

如果说前面的擦除只是类似LSTM的遗忘门，那么这里才是真正的写入新数据。

写磁头会生成一个**加数向量**$$\bm{a}_t$$，按权重$$w_t$$加在对应的结果上:

$$
\bm{M}_t(i)\longleftarrow \tilde{\bm{M}}_t(i)+w_t(i)\bm{a}_t
$$

跟按元素乘法一样，这里的加法也满足交换律，因而加的顺序不会造成影响。并且这两个操作都是可微的。

这里的一乘一加两个操作共用了权重$$\bm{w}_t$$，我简单地将其分量理解为：有多大的意愿修改存储中的这个条目。当其权重的在某个存储位置的分量较大时，将会尽可能用新的内容*取代*该位置上旧的存储内容。

### 寻址机制

事实上，读写头的权重向量$$\bm{w}_t$$是同一个控制器生成的。那么这个权重该如何产生呢？这就涉及到NTM的寻址机制。

NTM的寻址并不是固定某一个内存位置，而是通过设置$$\bm{w}_t$$各分量的大小，设置一次读写对不同位置的关注程度。作者将这个寻址过程描述为两步：

#### 基于内容寻址

每个读写头会输出一个长为$$M$$的*key vector* $\bm{k}_t$，然后将其与存储矩阵的每一个条目$$\bm{M}_t(i)$$计算相似度$$K[\cdot,\cdot]$$。根据相似度的大小做变换后归一化得到初步的权重（寻址偏好）：

$$
w^c_t(i)\longleftarrow\frac{\exp(\beta_tK[\bm{k}_t,\bm{M}_t(i)])}{\sum_j\exp(\beta_tK[\bm{k}_t,\bm{M}_t(j)])}
$$

这里使用将相似度按参数*key strength* $$\beta_t$$等比例放大后进行softmax，得到一个不同内存位置的概率分布。

key strength用于控制这一步寻址的聚焦程度，$$\beta_t$$越大权重就越集中。相似度采用的是余弦相似度。

$$
K[\bm{u},\bm{v}]=\frac{\bm{u}\cdot\bm{v}}{\|\bm{u\|\cdot\|}\bm{v\|}}
$$

#### 基于位置寻址

只使用基于内容寻址有一个明显的问题，就是对于任务的不同类型输入泛化性不会很好。假如训练了一个用于计算$$f(x,y)=x\times y$$的NTM，那么我们期望的是NTM总能从存储中的两个固定位置取出数据并且相乘，而与这两个位置具体存的内容没有关系。

##### 插值门

为了控制前一步中基于内容寻址的影响大小，引入*插值门*参数$$g_t\in(0,1)$$

$$
\bm{w}^g_t\longleftarrow g_t\bm{w}_t^c+(1-g_t)\bm{w}_{t-1}
$$

该参数将控制接下来的权重有多大程度来自于基于内容寻址或者上一时刻的权重。

##### 移位权重

图灵机的读写头是需要移动的，假设$$g_t$$总是很小，NTM的读写头也不能一直保持不动，因此需要一个方式描述究竟读写头要移动几位。

这里的设计是将读写头的移动看成是连续的，生成一个*移位向量* $$\bm{s}_t$$，假设需要移动6.7位，就设$$s_t(6)=0.3,s_t(7)=0.7$$，其余位为0。移位具体的计算方式是用一个循环卷积实现。

$$
\tilde{w}_t(i)\longleftarrow\sum_{j=0}^{N-1}w_t^g(j)s_t(i-j)
$$


完整的寻址流程如下图所示：

![ntm_write](/img/ntm_write.png)



# 参考文献

[^graves2014neural]: Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. *arXiv preprint arXiv:1410.5401*.