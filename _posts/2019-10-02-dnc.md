---
layout: post
title:  可微分神经计算机
date:   2019-10-02 
subtitle:   Differentiable Neural Computer
author:     CYQ
header-img: img/dnc.webp
header-mask: 0.2
catalog: true
mathjax: true
header-img-credit: DeepMind
header-img-credit-href: 'https://deepmind.com/blog/article/differentiable-neural-computers'
tags:
  - 深度学习
  - 神经图灵机
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

* TOC
{:toc}
------

可微分神经计算机(DNC)可以说是神经图灵机(NTM)的精神续作，并且造了个大新闻，上了Nature[^graves2016hybrid]。不得不说DeepMind出品，必属精品。

# 结构

论文中给出了如下图示：

![](/img/dnc_arch.png)

总体来说仍然延续了NTM的那一套设计理念：通过显式的对Memory操作来模拟图灵机的工作。下面将逐个部件地进行分析，同时与NTM进行比较。

##  工作原理

### 内存

跟NTM类似，组织成一个$$N\times W$$的矩阵，共$$N$$个条目，每个条目是长为$$W$$的一个向量。

### 控制器

相比NTM，这篇文章还算是把控制器结构说清楚了。文中使用了一个LSTM作为控制器。

LSTM每一时刻的输入由上一个时刻中，$R$个读磁头读出的数据$$\bm{r}_{t-1}^1,\dots,\bm{r}_{t-1}^R\in\mathbb{R}^W$$，和这一时刻新的输入$$\bm{x}_t\in\mathbb{R}^X$$连接而成。

$$
\bm{\chi}_t=[\bm{x}_t;\bm{r}_{t-1}^1;\dots;\bm{r}_{t-1}^R]
$$

假设LSTM有$$L$$个cell，那么第$$l$$个cell的各参数如下

$$
\begin{align}
\bm{i}^l_t&=\sigma(W_{\bm{i}}^l[\bm{\chi}_t;\bm{h}_{t-1}^l;\bm{h}_{t}^{l-1}]+\bm{b_i}^l)\\
\bm{f}_t^l&=\sigma(W_{\bm{f}}^l[\bm{\chi}_t;\bm{h}_{t-1}^l;\bm{h}_{t}^{l-1}]+\bm{b_f}^l)\\
\bm{s}_t^l&=\bm{f}_t^l\bm{s}_{t-1}^l+\bm{i}_t^l\tanh(W_{\bm{s}}^l[\bm{\chi}_t;\bm{h}_{t-1}^l;\bm{h}_t^{l-1}]+\bm{b_s}^l)\\
\bm{o}_t^l&=\sigma(W_{\bm{o}}^l[\bm{\chi}_t;\bm{h}_{t-1}^l;\bm{h}_{t}^{l-1}]+\bm{b_o}^l)\\
\bm{h}_t^l&=\bm{o}_t^l\tanh(\bm{s}_t^l)
\end{align}
$$

我们不关心LSTM的输出$$\bm{o}_t^l$$，而只使用其隐藏层的参数$$\bm{h}_t^l$$。

每个时刻，控制器根据隐藏层参数输出两个向量：**输出向量**$$\bm{\upsilon}_t$$，**接口向量(interface vector)**$$\bm{\xi}_t$$

$$
\begin{align}
\bm{\upsilon}_t&=W_{\bm{\upsilon}}[\bm{h}_{t}^{1};\dots;\bm{h}_{t}^{L}]\\
\bm{\xi}_t&=W_{\bm{\xi}}[\bm{h}_{t}^{1};\dots;\bm{h}_{t}^{L}]
\end{align}
$$

*输出向量*会直接参与这一时刻输出$$\bm{y}_t$$的生成过程

$$
\bm{y}_t=\bm{\upsilon}_t+W_r[\bm{r}_t^1;\dots;\bm{r}_t^R]
$$

而如何进行读写（与内存的交互），就全部由*接口向量*控制，因此接口向量中有大量的参数。

### 与内存的交互

#### 接口向量

接口向量$$\bm{\xi}_t\in\mathbb{R}^{(W\times R)+3W+5R+3}$$包含了接下来内存交互中所会用到的参数

$$
\bm{\xi}_t=\left[\bm{k}_t^{r,1};\dots;\bm{k}_t^{r,R};\hat{\beta}_t^{r,1};\dots;\hat{\beta}_t^{r,R};\bm{k}_t^w;\hat{\beta}_t^w;\bm{\hat{e}}_t;\bm{v}_t;\hat{f}_t^1;\dots;\hat{f}_t^R;\hat{g}_t^a,\hat{g}_t^w;\bm{\hat{\pi}}_t^1;\dots;\bm{\hat{\pi}}_t^R\right]
$$

- $$R$$个***read keys***：$$\{\bm{k}_t^{r,i}\in\mathbb{R}^W;1\leq i\leq R\}$$（NTM中读磁头的key vector）
- $$R$$个***read stength***：$$\{\beta_t^{r,i}=\text{oneplus}(\hat{\beta}_t^{r,i})\in[1,\infty);1\leq i\leq R\}$$（NTM中读磁头的key strength）
- ***write key***：$$\bm{k}_t^w\in\mathbb{R}^W$$（NTM中写磁头的key vector）
- ***write stength***：$$\{\beta_t^w=\text{oneplus}(\hat{\beta}_t^w)\in[1,\infty)\}$$（NTM中写磁头的key strength）
- **擦除向量**：$$\bm{e}_t=\sigma(\bm{\hat{e}}_t)\in[0,1]^W$$（同NTM）
- **写入向量**：$$\bm{v}_t\in\mathbb{R}^W$$（NTM中的加数向量）
- $$R$$个**释放门**：$$\{f_t^i=\sigma(\hat{f}_t^i)\in[0,1];1\leq i\leq R\}$$
- **分配门**：$$g_t^a=\sigma(\hat{g}_t^a)\in[0,1]$$
- $$R$$个**读模式**：$$\{\bm{\pi}_t^i=\text{softmax}(\bm{\hat{\pi}}_t^i)\in\mathcal{S}_3;1\leq i\leq R\}$$

其中
$$
\begin{align}
\text{oneplus}(x)&=1+\log(1+e^x)\\
\mathcal{S}_N&=\left\{\bm{\alpha}\in\mathbb{R}^N:\bm{\alpha}_i\in[0,1],\sum_{i=1}^N\bm{\alpha}_i=1\right\}
\end{align}
$$
与NTM相较，首先规定了只有一个写磁头，读磁头可以任意多，并且增加了一些“门”。读也有了几个模式。

另外，

# 参考文献

[^graves2016hybrid]: Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., ... & Badia, A. P. (2016). Hybrid computing using a neural network with dynamic external memory. *Nature*, *538*(7626), 471.