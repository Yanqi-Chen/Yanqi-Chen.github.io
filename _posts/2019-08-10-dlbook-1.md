---
layout: post
title:  深度学习花书笔记（二）
date:   2019-08-10 
subtitle:   机器学习基础
author:     CYQ
header-img: img/deep-learning-1.jpg
catalog: true
mathjax: true
tags:
  - 深度学习
typora-root-url: ..
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

# 估计、偏差与方差

对于模型的参数$\bm{\theta}$，$\lbrace\bm{x}^{(1)},\cdots,\bm{x}^{(m)}\rbrace$是$m$个独立同分布(i.i.d.)的数据点，$\bm{\theta}$的**点估计**是这些数据的任意一个函数

$$
\hat{\bm{\theta}}_m=g(\bm{x}^{(1)},\cdots,\bm{x}^{(m)})
$$

## 偏差

估计的偏差定义为

$$
\text{Bias}(\hat{\bm{\theta}}_m)=\mathbb{E}(\hat{\bm{\theta}}_m)-\bm{\theta}
$$

如果$\text{bias}(\hat{\bm{\theta}}_m)=0$，那么该估计量是**无偏**的。

如果$\lim_{m\to\infty}\text{bias}(\hat{\bm{\theta}}_m)=0$，那么该统计量是**渐进无偏**的。

## 关系

点估计的**均方误差**（MSE）恰好为方差与偏差的平方之和，这是因为

$$
\begin{split}
\text{MSE}&=\mathbb{E}[(\hat{\bm{\theta}}_m-\bm{\theta})^2]\\
&={\color{red}{\mathbb{E}[\hat{\bm{\theta}}_m^2]}}{\color{blue}{-2\bm{\theta}\mathbb{E}[\hat{\bm{\theta}}_m]+\bm{\theta}^2}}\\
&={\color{blue}{\left(\mathbb{E}^2[\hat{\bm{\theta}}_m]-2\bm{\theta}\mathbb{E}[\hat{\bm{\theta}}_m]+\bm{\theta}^2\right)}}+{\color{red}{\left(\mathbb{E}[\hat{\bm{\theta}}_m^2]-\mathbb{E}^2[\hat{\bm{\theta}}_m]\right)}}\\
&={\color{blue}{\text{Bias}(\hat{\bm{\theta}}_m)^2}}+{\color{red}{\text{Var}(\hat{\bm{\theta}}_m)}}
\end{split}
$$

因此我们需要权衡方差与偏差

![bias-var](/img/bias-var.svg)

# 最大似然估计（频率派）

频率派的视角为：参数$\bm{\theta}$存在一个**真实但未知的定值**。

含$m$个样本的数据集$\mathbb{X}=\lbrace\bm{x}^{(1)},\cdots,\bm{x}^{(m)}\rbrace$独立地由未知的真实数据生成分布$p_{\text{data}}(\bm{x})$生成。

设模型$p_{\text{model}}(\bm{x};\bm{\theta})$是一族含参数$\bm{\theta}$的概率分布，我们希望从中找到一个分布，使得它生成$\mathbb{X}$的概率最大化，从而求出最合适的$\bm{\theta}$，这个参数值记为**最大似然估计** $\bm{\theta}_{\text{ML}}$

$$
\begin{split}
\bm{\theta}_{\text{ML}}&=\mathop{\arg\max}_{\bm{\theta}}p_{\text{model}}(\mathbb{X};\bm{\theta})\\
&=\mathop{\arg\max}_{\bm{\theta}}\prod_{i=1}^mp_{\text{model}}(\bm{x}^{(i)};\bm{\theta})
\end{split}
$$

实际为了计算方便，以及消除乘积可能的数值上溢下溢等问题，会对似然求对数，这个操作不会改变$\arg\max$的结果。

$$
\bm{\theta}_{\text{ML}}=\mathop{\arg\max}_{\bm{\theta}}\sum_{i=1}^m\log p_{\text{model}}(\bm{x}^{(i)};\bm{\theta})\tag{1}\label{ML}
$$

另一个视角是将$\eqref{ML}$进一步变形(PS：将光标放在公式等号处查看原因)

$$
\begin{split}
\bm{\theta}_{\text{ML}}&=\mathop{\arg\max}_{\bm{\theta}}\sum_{i=1}^m\log p_{\text{model}}(\bm{x}^{(i)};\bm{\theta})\\
&\texttip{=}{样本数量的对数被丢弃}\mathop{\arg\max}_{\bm{\theta}}\mathbb{E}_{x\sim \hat{p}_{\text{data}}}\left[\log p_{\text{model}}(\bm{x}^{(i)};\bm{\theta})\right]\\
&\mathtip{=}{\text{新增与}p_{\text{model}}\text{无关的项}}\mathop{\arg\max}_{\bm{\theta}}\left(-\mathbb{E}_{x\sim \hat{p}_{\text{data}}}\left[\log \hat{p}_{\text{data}}(\bm{x}^{(i)})-\log p_{\text{model}}(\bm{x}^{(i)};\bm{\theta})\right]\right)\\
&=\mathop{\arg\min}_{\bm{\theta}}D_{\text{KL}}(\hat{p}_{\text{data}}\|p_{\text{model}})
\end{split}
$$

因此最大似然实际上是在最小化模型$p_ {\text{model}}$与经验分布$\hat{p}_ {\text{data}}$的KL散度，亦即最小化两者的交叉熵。（注意:warning:：经验分布$\hat{p}_ {\text{data}}\neq p_ {\text{data}}$真实分布）

如果满足如下两个条件，最大似然估计会收敛到参数的真实值

- 真实分布$p_{\text{data}}$在模型族$p_{\text{model}}(\cdot;\bm{\theta})$中
- 只有一个$\bm{\theta}$的值对应真实分布$p_{\text{data}}$，否则即使估计出真实分布$p_{\text{data}}$，也无法确认使用哪一个$\bm{\theta}$

# 最大后验估计（贝叶斯派）

贝叶斯派的观点为：参数$\bm{\theta}$是**随机**的，满足某个概率分布。

根据贝叶斯公式

$$
p(\bm{\theta\mid x})=\frac{p(\bm{x\mid \theta})p(\bm{\theta})}{p(\bm{x})}
$$

最大化上式得到**最大后验点估计** $\bm{\theta}_{\text{MAP}}$

$$
\bm{\theta}_{\text{MAP}}=\mathop{\arg\max}_{\bm{\theta}}p(\bm{\theta\mid x})=\mathop{\arg\max}_{\bm{\theta}}\left\{\log p(\bm{x\mid \theta})+\log p(\bm{\theta})\right\}
$$

如果某些正则化项恰好是一个概率分布的对数，我们就可以将其模型解释为在这个先验概率分布下的最大似然估计。

# PCA的另一种视角

对于设计矩阵$\bm{X}_{m\times n}$，数据的均值为0（已中心化），即$\mathbb{E}[\bm{x}]=0$。$\bm{X}$的无偏样本协方差矩阵为

$$
\text{Var}[\bm{x}]=\frac{1}{m-1}\bm{X}^\top\bm{X}
$$

PCA通过一个线性变换$\bm{z}=\bm{W}^\top\bm{x}$使得$\text{Var}[\bm{z}]$为对角阵。

除去特征分解的做法，我们还可以使用SVD得到$\bm{W}$

设$\bm{W}$是奇异值分解$\bm{X}=\bm{U\Sigma W}^\top$的右奇异向量，我们就用这个作为线性变换的基。那么，原始的协方差可表示为

$$
\begin{split}
\text{Var}[\bm{x}]&=\frac{1}{m-1}\bm{X}^\top\bm{X}\\
&=\frac{1}{m-1}\left(\bm{U\Sigma W}^\top\right)^\top \bm{U\Sigma W}^\top\\
&=\frac{1}{m-1}\bm{W\Sigma}^\top\bm{U}^\top \bm{U\Sigma W}^\top\\
&\mathtip{=}{\bm{U}\text{为正交阵},\bm{U}^\top\bm{U}=\bm{I}}\frac{1}{m-1}\bm{W\Sigma}^2\bm{W}^\top
\end{split}\label{pca}\tag{2}
$$

接下来我们计算变换后的协方差矩阵$\text{Var}[\bm{z}]$

$$
\begin{split}
\text{Var}[\bm{z}]&=\frac{1}{m-1}\bm{W}^\top \bm{X}^\top\bm{XW}\\
&\stackrel{\eqref{pca}}=\frac{1}{m-1}\bm{W}^\top\bm{W\Sigma}^2\bm{W}^\top\bm{W}\\
&\mathtip{=}{\bm{W}\text{为正交阵},\bm{W}^\top\bm{W}=\bm{I}}\frac{1}{m-1}\bm{\Sigma}^2
\end{split}
$$

得到的恰好是$\bm{X}^\top\bm{X}$的若干特征值的对角阵。

